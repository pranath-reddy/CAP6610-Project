{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17130,"status":"ok","timestamp":1681401321316,"user":{"displayName":"Pranath Reddy Kumbam","userId":"07194914569693395860"},"user_tz":240},"id":"uV8Lch-wnW32","outputId":"19a3e79c-0074-4aaf-f9fe-bd9b46304152"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1681401321317,"user":{"displayName":"Pranath Reddy Kumbam","userId":"07194914569693395860"},"user_tz":240},"id":"Fltk46feoNyW","outputId":"3ed2c74a-edae-4d2d-ca63-da1c6542fd38"},"outputs":[],"source":["cd drive/My \\Drive/ML/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12069,"status":"ok","timestamp":1681401333573,"user":{"displayName":"Pranath Reddy Kumbam","userId":"07194914569693395860"},"user_tz":240},"id":"q41YaVjL2E3Q","outputId":"c2988a5c-10f1-4f51-fe3a-33aad3927b0d"},"outputs":[],"source":["!pip install pytorch_fid\n","!pip install POT"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":573082,"status":"ok","timestamp":1681401906652,"user":{"displayName":"Pranath Reddy Kumbam","userId":"07194914569693395860"},"user_tz":240},"id":"pcIcBleT0-z2","outputId":"1d0af7cc-75c1-4a68-b317-b26c7e184f53"},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torchvision.models import inception_v3\n","from pytorch_fid import fid_score\n","from scipy.stats import entropy\n","import torchvision\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from torchvision.transforms.functional import resize\n","import ot\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Generator\n","latent_dim = 100\n","channels = 1\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        self.main = nn.Sequential(\n","            nn.ConvTranspose2d(latent_dim, 64 * 4, 4, 1, 0, bias=False),\n","            nn.BatchNorm2d(64 * 4),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(64 * 4, 64 * 2, 3, 2, 1, bias=False),\n","            nn.BatchNorm2d(64 * 2),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(64 * 2, 64, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(64, channels, 4, 2, 1, bias=False),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        return self.main(x)\n","\n","# Load the trained GAN models\n","print('Loading pre-trained model')\n","generator = Generator().to(device)\n","generator.load_state_dict(torch.load(\"./Weights/MNIST_GAN_generator.pth\"))\n","\n","# Fr√©chet Inception Distance (FID)\n","print('Calculating FID')\n","def generate_fake_images(num_images):\n","    noise = torch.randn(num_images, latent_dim, 1, 1, device=device)\n","    fake_images = generator(noise)\n","    # Convert grayscale images to RGB by duplicating the single channel three times\n","    fake_images_rgb = fake_images.repeat(1, 3, 1, 1)\n","    return fake_images_rgb\n","\n","def extract_real_images(data_loader, num_images, save_dir=\"./data/MNIST/real_images\"):\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    real_images = []\n","    count = 0\n","    for images, _ in data_loader:\n","        for image in images:\n","            if count < num_images:\n","                torchvision.utils.save_image(image, f\"{save_dir}/image_{count}.png\")\n","                real_images.append(image)\n","                count += 1\n","            else:\n","                break\n","        if count >= num_images:\n","            break\n","\n","    return torch.stack(real_images)\n","\n","# MNIST dataset\n","batch_size = 128\n","image_size = 28\n","channels = 1\n","transform = transforms.Compose([\n","    transforms.Resize(image_size),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","mnist_data = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n","data_loader = DataLoader(mnist_data, batch_size=batch_size, shuffle=True)\n","\n","print('Generating Data')\n","num_images = 1000\n","real_images = extract_real_images(data_loader, num_images)\n","fake_images = generate_fake_images(num_images)\n","\n","# Save generated images\n","os.makedirs(\"./data/MNIST/fake_images\", exist_ok=True)\n","for idx, img in enumerate(fake_images):\n","    torchvision.utils.save_image(img, f\"./data/MNIST/fake_images/image_{idx}.png\")\n","\n","def calculate_fid(real_images_path, fake_images_path, batch_size=128):\n","    dims = 2048  # Set to 2048 for Inception v3\n","    fid = fid_score.calculate_fid_given_paths([real_images_path, fake_images_path], batch_size, device, dims)\n","    return fid\n","\n","# paths for real and fake images\n","real_images_path = \"./data/MNIST/real_images\"\n","fake_images_path = \"./data/MNIST/fake_images\"\n","\n","# Calculate FID\n","fid = calculate_fid(real_images_path, fake_images_path)\n","print(\"FID:\", fid)\n","\n","# EMD\n","print('Calculating EMD')\n","print('Generating Data')\n","num_images = 1000\n","real_images = extract_real_images(data_loader, num_images)\n","fake_images = generate_fake_images(num_images)\n","# Convert fake images back to grayscale\n","fake_images = fake_images[:, 0, :, :] * 0.299 + fake_images[:, 1, :, :] * 0.587 + fake_images[:, 2, :, :] * 0.114\n","\n","def calculate_emd(real_images, fake_images):\n","    real_images = real_images.view(real_images.size(0), -1).detach().cpu().numpy()\n","    fake_images = fake_images.view(fake_images.size(0), -1).detach().cpu().numpy()\n","    cost_matrix = ot.dist(real_images, fake_images)\n","    emd = ot.emd2([], [], cost_matrix)\n","    return emd\n","emd = calculate_emd(real_images, fake_images)\n","print(\"EMD:\", emd)\n","\n","# PRF Metrics\n","print('Calculating PRF')\n","def extract_features(images, model, batch_size=32):\n","    model.eval()  # Set the model to evaluation mode\n","    images = images.to(device)  # Move images to the same device as the model\n","    \n","    # Convert grayscale images to RGB by duplicating the single channel three times\n","    images_rgb = images.repeat(1, 3, 1, 1)\n","    \n","    # Resize images to match the Inception model's input size\n","    resize_transform = transforms.Resize((299, 299))\n","    images_rgb_resized = torch.stack([resize_transform(img) for img in images_rgb])\n","\n","    num_images = len(images_rgb_resized)\n","    num_batches = (num_images + batch_size - 1) // batch_size\n","    features_list = []\n","\n","    with torch.no_grad():  # Disable gradient computation to save memory\n","        for i in range(num_batches):\n","            start_idx = i * batch_size\n","            end_idx = min(start_idx + batch_size, num_images)\n","            batch_images = images_rgb_resized[start_idx:end_idx]\n","            batch_features = model(batch_images).squeeze()\n","            features_numpy = batch_features.cpu().numpy()\n","            features_list.append(features_numpy)\n","\n","    return np.concatenate(features_list, axis=0)\n","\n","# Load pre-trained Inception v3 model\n","inception = inception_v3(pretrained=True, transform_input=True).to(device)\n","inception.eval()\n","\n","print('Generating Data')\n","fake_images = fake_images.reshape(-1,1,28,28)\n","real_features = extract_features(real_images, inception)\n","fake_features = extract_features(fake_images, inception)\n","\n","# Train a classifier\n","X = np.concatenate((real_features, fake_features))\n","y = np.concatenate((np.ones(len(real_features)), np.zeros(len(fake_features))))\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","clf = RandomForestClassifier(random_state=0, n_jobs=-1)  # n_jobs=-1 uses all available CPU cores\n","clf.fit(X_train, y_train)\n","\n","# Predict probabilities\n","y_pred_proba = clf.predict_proba(X_test)\n","\n","# Choose the probability of the positive class as the prediction score\n","y_pred_scores = y_pred_proba[:, 1]\n","\n","# Calculate precision, recall, and F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred_scores > 0.5, average='binary')\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-score:\", f1_score)\n","\n","# Inception Score (IS)\n","print('Calculating IS')\n","def generate_fake_images(num_images):\n","    noise = torch.randn(num_images, latent_dim, 1, 1, device=device)\n","    fake_images = generator(noise)\n","    # Convert grayscale images to RGB by duplicating the single channel three times\n","    fake_images_rgb = fake_images.repeat(1, 3, 1, 1)\n","    # Resize images to 299x299\n","    fake_images_rgb_resized = torch.zeros(num_images, 3, 299, 299, device=device)\n","    for idx, img in enumerate(fake_images_rgb):\n","        fake_images_rgb_resized[idx] = resize(img, (299, 299))\n","    return fake_images_rgb_resized\n","\n","def inception_score(images, n_splits=10):\n","    # Load pre-trained Inception model\n","    inception_model = inception_v3(pretrained=True, transform_input=True).to(device)\n","    inception_model.eval()\n","\n","    # Calculate inception score\n","    scores = []\n","    n_total = images.shape[0]\n","    chunk_size = n_total // n_splits\n","    for k in range(n_splits):\n","        images_chunk = images[k * chunk_size: (k + 1) * chunk_size]\n","        with torch.no_grad():\n","            logits = inception_model(images_chunk)\n","        p_yx = torch.softmax(logits, dim=1).cpu().numpy()\n","        p_y = np.mean(p_yx, axis=0)\n","        scores.append(entropy(p_yx).mean() - entropy(p_y))\n","    return np.exp(np.mean(scores))\n","\n","print('Generating Data')\n","fake_images = generate_fake_images(1000)\n","inception_score = inception_score(fake_images)\n","print(\"Inception Score:\", inception_score)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
